# Multilingual Fine-Tuning of LLMs for Code-Mixed Text Understanding

## 🎯 Project Goal
This project explores fine-tuning multilingual language models (mBERT, XLM-RoBERTa) on **code-mixed text** (e.g., Urdu-English).  
The aim is to evaluate how well these models can handle multilingual, mixed-script input for tasks such as **sentiment analysis**.

## 📌 Planned Milestones
1. Dataset collection & preprocessing  
2. Baseline model (English-only)  
3. Fine-tune multilingual models  
4. Evaluation & comparison  
5. Streamlit/Flask demo  
6. Blog/report documentation  

## 🛠️ Tools
- Python, Hugging Face Transformers, Datasets
- PyTorch
- Jupyter/VS Code
- Streamlit (demo)

---

*This project is part of my Master’s application portfolio (NLP/LLM focus).*
