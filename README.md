# Multilingual Fine-Tuning of LLMs for Code-Mixed Text Understanding

## ğŸ¯ Project Goal
This project explores fine-tuning multilingual language models (mBERT, XLM-RoBERTa) on **code-mixed text** (e.g., Urdu-English).  
The aim is to evaluate how well these models can handle multilingual, mixed-script input for tasks such as **sentiment analysis**.

## ğŸ“Œ Planned Milestones
1. Dataset collection & preprocessing  
2. Baseline model (English-only)  
3. Fine-tune multilingual models  
4. Evaluation & comparison  
5. Streamlit/Flask demo  
6. Blog/report documentation  

## ğŸ› ï¸ Tools
- Python, Hugging Face Transformers, Datasets
- PyTorch
- Jupyter/VS Code
- Streamlit (demo)

---

*This project is part of my Masterâ€™s application portfolio (NLP/LLM focus).*
